{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Use the same data as in assignment 1 but this time identify top-10 tokens that occur in regulation descriptions in the table.\n",
    "1. As in assignment 1, extract regulation descriptions from each record corresponding to a failed inspection\n",
    "2. Tokenize each regulation description\n",
    "3. Find top-10 tokens (for the whole table)\n",
    "4. Clean data: convert to lower case, remove stopwords, punctuation, numbers, etc\n",
    "5. Find top-10 tokens again\n",
    "6. Find top-10 tokens after applying Porter stemming to the tokens obtained in step 4.\n",
    "7. Find top-10 tokens after applying Lancaster stemming to the tokens obtained in step 4.\n",
    "8. Find top-10 tokens after applying lemmatization to the tokens obtained in step 4.\n",
    "9. Compare top-10 tokens obtained in 3, 5, 6, 7, 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Loading Packages###\n",
    "import pandas as pd  # data frame operations  \n",
    "import numpy as np  # arrays and math functions\n",
    "\n",
    "import re # regular expressions\n",
    "import os # Operation System\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk as nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "food = pd.read_csv('Food_Inspections.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract regulation descriptions from each record corresponding to a failed inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = food[food['Results'] == 'Fail']\n",
    "failed_df = failed_df.dropna(subset=['Violations']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term to split on\n",
    "split_term = '\\|'\n",
    "# split the reasons\n",
    "failed_df['splitted_reasons'] = failed_df['Violations'].apply(lambda x: re.split(split_term, x))\n",
    "failed_df['descriptions'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find patterns\n",
    "comment = \"\\s\\-\\sComments:[\\s\\S]*\"\n",
    "regulation_code = \"[\\0-9]+\\.\\s\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(failed_df.shape[0]):\n",
    "    reasons = failed_df.iloc[i]['splitted_reasons']\n",
    "    \n",
    "    description = []\n",
    "    for r in reasons:\n",
    "        # delete comment\n",
    "        no_comment = re.sub(comment, '', r)\n",
    "\n",
    "        # delete regulation code\n",
    "        no_code = re.sub(regulation_code, '', no_comment)\n",
    "        no_code = no_code.strip()\n",
    "        description.append(no_code)\n",
    "    \n",
    "#     print(\"--------Row:\", i)\n",
    "#     print(description)\n",
    "    result.append(description)\n",
    "\n",
    "failed_df['descriptions'] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize each regulation description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in result for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_result = ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.tokenize.word_tokenize(str_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find top-10 tokens (for the whole table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 380 samples and 3302539 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 341824),\n",
       " ('AND', 165729),\n",
       " (':', 123154),\n",
       " ('MAINTAINED', 80448),\n",
       " ('FOOD', 78467),\n",
       " ('EQUIPMENT', 63910),\n",
       " ('CONSTRUCTED', 63844),\n",
       " ('CLEAN', 62129),\n",
       " ('PROPERLY', 62125),\n",
       " ('OF', 60053)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "print(fdist)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, freq = zip(*fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_list)\n",
    "freq = list(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean data: convert to lower case, remove stopwords, punctuation, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# Remove punctuation\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "words_lc = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords\n",
    "words_lc = [word for word in words_lc if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find top-10 tokens again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 325 samples and 2175835 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('maintained', 80448),\n",
       " ('food', 78467),\n",
       " ('equipment', 63910),\n",
       " ('constructed', 63844),\n",
       " ('clean', 62129),\n",
       " ('properly', 62125),\n",
       " ('installed', 59289),\n",
       " ('cleaning', 48295),\n",
       " ('surfaces', 44693),\n",
       " ('contact', 42211)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_cleaned = nltk.FreqDist(words_lc)\n",
    "\n",
    "print(fdist_cleaned)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist_cleaned.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_cleaned, freq_cleaned = zip(*fdist_cleaned.most_common(10))\n",
    "word_list_cleaned = list(word_list_cleaned)\n",
    "freq_cleaned = list(freq_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Find top-10 tokens after applying Porter stemming to the tokens obtained in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "word_porter_stem = [porter.stem(t) for t in words_lc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 282 samples and 2175835 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('clean', 135055),\n",
       " ('maintain', 83997),\n",
       " ('food', 83723),\n",
       " ('equip', 63910),\n",
       " ('construct', 63844),\n",
       " ('properli', 62125),\n",
       " ('instal', 59289),\n",
       " ('surfac', 44693),\n",
       " ('contact', 42211),\n",
       " ('method', 40605)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_porter = nltk.FreqDist(word_porter_stem)\n",
    "\n",
    "print(fdist_porter)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist_porter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_porter, freq_porter = zip(*fdist_porter.most_common(10))\n",
    "word_list_porter = list(word_list_porter)\n",
    "freq_porter = list(freq_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find top-10 tokens after applying Lancaster stemming to the tokens obtained in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "word_lancaster_stem = [lancaster.stem(t) for t in words_lc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 270 samples and 2175835 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cle', 141144),\n",
       " ('maintain', 83997),\n",
       " ('food', 83723),\n",
       " ('prop', 76405),\n",
       " ('equip', 63910),\n",
       " ('construct', 63844),\n",
       " ('instal', 59289),\n",
       " ('surfac', 44693),\n",
       " ('contact', 42211),\n",
       " ('method', 40605)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_lancaster = nltk.FreqDist(word_lancaster_stem)\n",
    "\n",
    "print(fdist_lancaster)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist_lancaster.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_lancaster, freq_lancaster = zip(*fdist_lancaster.most_common(10))\n",
    "word_list_lancaster = list(word_list_lancaster)\n",
    "freq_lancaster = list(freq_lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find top-10 tokens after applying lemmatization to the tokens obtained in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "word_lemmatize = [wnl.lemmatize(t) for t in words_lc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 309 samples and 2175835 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('food', 83723),\n",
       " ('maintained', 80448),\n",
       " ('equipment', 63910),\n",
       " ('constructed', 63844),\n",
       " ('clean', 62129),\n",
       " ('properly', 62125),\n",
       " ('installed', 59289),\n",
       " ('cleaning', 48295),\n",
       " ('surface', 44693),\n",
       " ('contact', 42211)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_lemma = nltk.FreqDist(word_lemmatize)\n",
    "\n",
    "print(fdist_lemma)\n",
    "\n",
    "#fdist.items() - will give all words\n",
    "fdist_lemma.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_lemma, freq_lemma = zip(*fdist_lemma.most_common(10))\n",
    "word_list_lemma = list(word_list_lemma)\n",
    "freq_lemma = list(freq_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare top-10 tokens obtained in 3, 5, 6, 7, 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(list(zip(word_list, freq, word_list_cleaned, freq_cleaned,\n",
    "                               word_list_porter, freq_porter,\n",
    "                               word_list_lancaster, freq_lancaster,\n",
    "                               word_list_lemma, freq_lemma)),\n",
    "               columns =['Whole', 'Freq', 'Cleaned', 'Freq', \n",
    "                         'Porter', 'Freq', 'Lancaster', 'Freq', 'Lemma','Freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Whole</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Cleaned</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Porter</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Lancaster</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>341824</td>\n",
       "      <td>maintained</td>\n",
       "      <td>80448</td>\n",
       "      <td>clean</td>\n",
       "      <td>135055</td>\n",
       "      <td>cle</td>\n",
       "      <td>141144</td>\n",
       "      <td>food</td>\n",
       "      <td>83723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AND</td>\n",
       "      <td>165729</td>\n",
       "      <td>food</td>\n",
       "      <td>78467</td>\n",
       "      <td>maintain</td>\n",
       "      <td>83997</td>\n",
       "      <td>maintain</td>\n",
       "      <td>83997</td>\n",
       "      <td>maintained</td>\n",
       "      <td>80448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:</td>\n",
       "      <td>123154</td>\n",
       "      <td>equipment</td>\n",
       "      <td>63910</td>\n",
       "      <td>food</td>\n",
       "      <td>83723</td>\n",
       "      <td>food</td>\n",
       "      <td>83723</td>\n",
       "      <td>equipment</td>\n",
       "      <td>63910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAINTAINED</td>\n",
       "      <td>80448</td>\n",
       "      <td>constructed</td>\n",
       "      <td>63844</td>\n",
       "      <td>equip</td>\n",
       "      <td>63910</td>\n",
       "      <td>prop</td>\n",
       "      <td>76405</td>\n",
       "      <td>constructed</td>\n",
       "      <td>63844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>78467</td>\n",
       "      <td>clean</td>\n",
       "      <td>62129</td>\n",
       "      <td>construct</td>\n",
       "      <td>63844</td>\n",
       "      <td>equip</td>\n",
       "      <td>63910</td>\n",
       "      <td>clean</td>\n",
       "      <td>62129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EQUIPMENT</td>\n",
       "      <td>63910</td>\n",
       "      <td>properly</td>\n",
       "      <td>62125</td>\n",
       "      <td>properli</td>\n",
       "      <td>62125</td>\n",
       "      <td>construct</td>\n",
       "      <td>63844</td>\n",
       "      <td>properly</td>\n",
       "      <td>62125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CONSTRUCTED</td>\n",
       "      <td>63844</td>\n",
       "      <td>installed</td>\n",
       "      <td>59289</td>\n",
       "      <td>instal</td>\n",
       "      <td>59289</td>\n",
       "      <td>instal</td>\n",
       "      <td>59289</td>\n",
       "      <td>installed</td>\n",
       "      <td>59289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CLEAN</td>\n",
       "      <td>62129</td>\n",
       "      <td>cleaning</td>\n",
       "      <td>48295</td>\n",
       "      <td>surfac</td>\n",
       "      <td>44693</td>\n",
       "      <td>surfac</td>\n",
       "      <td>44693</td>\n",
       "      <td>cleaning</td>\n",
       "      <td>48295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PROPERLY</td>\n",
       "      <td>62125</td>\n",
       "      <td>surfaces</td>\n",
       "      <td>44693</td>\n",
       "      <td>contact</td>\n",
       "      <td>42211</td>\n",
       "      <td>contact</td>\n",
       "      <td>42211</td>\n",
       "      <td>surface</td>\n",
       "      <td>44693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OF</td>\n",
       "      <td>60053</td>\n",
       "      <td>contact</td>\n",
       "      <td>42211</td>\n",
       "      <td>method</td>\n",
       "      <td>40605</td>\n",
       "      <td>method</td>\n",
       "      <td>40605</td>\n",
       "      <td>contact</td>\n",
       "      <td>42211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Whole    Freq      Cleaned   Freq     Porter    Freq  Lancaster  \\\n",
       "0            ,  341824   maintained  80448      clean  135055        cle   \n",
       "1          AND  165729         food  78467   maintain   83997   maintain   \n",
       "2            :  123154    equipment  63910       food   83723       food   \n",
       "3   MAINTAINED   80448  constructed  63844      equip   63910       prop   \n",
       "4         FOOD   78467        clean  62129  construct   63844      equip   \n",
       "5    EQUIPMENT   63910     properly  62125   properli   62125  construct   \n",
       "6  CONSTRUCTED   63844    installed  59289     instal   59289     instal   \n",
       "7        CLEAN   62129     cleaning  48295     surfac   44693     surfac   \n",
       "8     PROPERLY   62125     surfaces  44693    contact   42211    contact   \n",
       "9           OF   60053      contact  42211     method   40605     method   \n",
       "\n",
       "     Freq        Lemma   Freq  \n",
       "0  141144         food  83723  \n",
       "1   83997   maintained  80448  \n",
       "2   83723    equipment  63910  \n",
       "3   76405  constructed  63844  \n",
       "4   63910        clean  62129  \n",
       "5   63844     properly  62125  \n",
       "6   59289    installed  59289  \n",
       "7   44693     cleaning  48295  \n",
       "8   42211      surface  44693  \n",
       "9   40605      contact  42211  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- The top-10 tokens from the original data contain some punctuations, which are meaningless and not what we want.\n",
    "- The result from cleaned data and lemmatized data are very similar. One difference is that they both contain the word \"food\" and \"maintained\" but with different frequencies. The reason for this might be that some of the sub-categories(foods, maintains, maintaining, etc) are combined into the top 2 token in the lemmatized result.\n",
    "- 2 stemmed results are similar as well. But some of the words lose their original meaning. For example, the \"properli\", \"instal\", \"surfac\" in the result from Porter stemming, and the \"cle\" from Lancaster stemming make little sense. Even though we could guess some of their meanings under this specific circumstance, these 2 are still not the optimal results. \n",
    "- From the frequency lists, we could see that \"clean\" and \"cleaning\" are combined into 1 token in stemmed results but not in the cleaned result and the lemmatized result. \n",
    "- Overall, the lemmatized result gives a more reasonable top-10 tokens list with words having proper meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
